---
title: "Introduction to the comparer R package"
author: "Collin Erickson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to the comparer R package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo=FALSE}
set.seed(0)
```


When coding, especially for data science, there are multiple
ways to solve each problem.
When presented with two options, you want to pick the one
that is faster and/or more accurate.
Comparing different code chunks on the same task can be tedious.
It often requires creating data, writing a for loop
(or using `sapply`), then comparing.

The comparer package makes this comparison quick and simple:

* The same data can be given in to each model.

* Various metrics can be used to judge results,
including using the predicted errors from the code.

* The results are displayed in a table that allows you
to quickly judge the results.

This document introduces the main function of the `comparer` package, `mbc`.


## Motivation from `microbenchmark`

The R package `microbenchmark` provides the fantastic eponymous function.
It makes it simple to run different segments of code and see which is faster.
Borrowing an example from http://adv-r.had.co.nz/Performance.html,
the following shows how it gives a summary of how fast each ran.

```{r}
library(microbenchmark)
x <- runif(100)
microbenchmark(sqrt(x), x ^ .5)
```

However it gives no summary of the output.
For this example it is fine since the output is deterministic,
but when working with randomness or model predictions we want
to have some sort of summary or evaluation metric to see which
has better accuracy, or to just see how the outputs differ.


## `mbc` to the rescue

The function `mbc` in the `comparer` package was created to solve this
problem, where a comparison of the output is desired in addition to
the run time.

For example, we may wish to see how the sample size affects
an estimate of the mean of a random sample.
The following shows the results of finding the mean of 10 and 100
samples from a normal distribution.

```{r}
library(comparer)
mbc(mean(rnorm(10)), mean(rnorm(100)))
```

By default it only runs 5 trials, but this can be changed with the
`times` parameter.
The first part of the output gives the run times.
For 5 or fewer, it shows all the values in sorted order,
for more than 5 it shows summary statistics.
Unfortunately, the timing is only accurate up to 0.01 seconds,
so these all show as 0.

The second section of the output gives the summary of the output.
This also will show summary stats for more than 5 trials,
but for this small sample size it shows all the values in sorted order
with the mean and standard deviation given.
The first column shows the name of each,
and the second column shows which output statistic is given.
Since there is only one output for this code it is called "1".

Setting `times` changes the number of trials run.
Below the same example as above is run but for 100 trials.

```{r}
mbc(mean(rnorm(10)), mean(rnorm(100)), times=100)
```

We see that the mean of both is around zero,
but that the larger sample size (`mean(rnorm(100))`) has a tighter
distribution and a standard deviation a third as large as the other,
which is about what we expect for a sample that is 10 times larger
(it should be $\sqrt{10} \approx 3.16$ times smaller on average).

In this example each function had its own input,
but many times we want to compare the functions
on the same input for better comparison.


## Shared input

Input can be passed in to the `input` argument as a list,
and then the code will be evaluated in an environment with that data.
In this example we compare the functions `mean` and `median` on random
data from an exponential distribution.
The mean should be about 1, while the median should be about
$\ln(2)=0.693$.

```{r}
mbc(mean(x), median(x), input=list(x=rexp(30)))
```

In this case each evaluation is identical since the input is not random.
The data passed to `input` is kept as is, so there is no randomness
from the data.
If we want randomness in the data, we can use `inputi`,
which evaluates its argument as an expression,
meaning that each time it will be different.

Below is the same code as above except `inputi` is used with
`x` set in brackets instead of a list.
We see there is randomness and we can get an idea of the distribution
of the median and mean.

```{r}
mbc(mean(x), median(x), inputi={x=rexp(30)})
```

When the code chunks to evaluate are simple functions of a single variable,
this can be simplified.
Look how simple it is to run a test on these!

```{r}
mbc(mean, median, inputi=rexp(30))
```

## Comparing to expected values

The previous comparisons showed a summary of the outputs,
but many times we want to compare output values to true values,
then calculate a summary statistic, such as an average error.
The argument `target` specifies the values the code chunks should
give, then summary statistics can be calculated by specifying `metrics`,
which defaults to calculating the rmse.

For example, suppose we have data from a linear function,
and want to see how accurate the model is when the output
values are corrupted with noise.
Below we compare two linear models: the first with an intercept term,
and the second without.
The model with the intercept term should be much better
since the data has an intercept of $-0.6$.
We see that the output is different in a few ways now.
The `Stat`

```{r}
n <- 20
x <- seq(0, 1, length.out = n)
y <- 1.8 * x - .6
ynoise <- y + rnorm(n, 0, .2)
```

```{r}
mbc(predict(lm(ynoise ~ x), data.frame(x)),
    predict(lm(ynoise ~ x - 1), data.frame(x)),
    target = y)
```



